{"cells":[{"cell_type":"markdown","metadata":{"id":"QVfvjQi5czOs"},"source":["# Lesson 4.张量的线性代数运算"]},{"cell_type":"markdown","metadata":{"id":"CPKPts5dczOz"},"source":["&emsp;&emsp;也就是PyTorch中BLAS和LAPACK模块的相关运算。"]},{"cell_type":"markdown","metadata":{"id":"aCBtT6eDczO0"},"source":["&emsp;&emsp;PyTorch中并未设置单独的矩阵对象类型，因此PyTorch中，二维张量就相当于矩阵对象，并且拥有一系列线性代数相关函数和方法。      \n","&emsp;&emsp;在实际机器学习和深度学习建模过程中，矩阵或者高维张量都是基本对象类型，而矩阵所涉及到的线性代数理论也是深度学习用户必备的基本数学基础。因此，本节在介绍张量的线性代数运算时，也会回顾基本的矩阵运算，及其基本线性代数的数学理论基础，以期在强化张量的线性代数运算过程中，也进一步夯实同学的线性代数数学基础。      \n","&emsp;&emsp;另外，在实际的深度学习建模过程中，往往会涉及矩阵的集合，也就是三维甚至是四维张量的计算，因此在部分场景中，我们也将把二维张量计算拓展到更高维的张量计算。"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Cyr_7iQ4czO1","executionInfo":{"status":"ok","timestamp":1648570365864,"user_tz":-120,"elapsed":7467,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"QZA8BP3_czO2"},"source":["## 一、BLAS和LAPACK概览"]},{"cell_type":"markdown","metadata":{"id":"W-WRZxR1czO3"},"source":["&emsp;&emsp;BLAS（Basic Linear Algeria Subprograms）和LAPACK（Linear Algeria Package）模块提供了完整的线性代数基本方法，由于涉及到函数种类较多，因此此处对其进行简单分类，具体包括：\n","- 矩阵的形变及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上/下三角矩阵的创建等；\n","- 矩阵的基本运算：包括矩阵乘法、向量内积、矩阵和向量的乘法等，当然，此处还包含了高维张量的基本运算，将着重探讨矩阵的基本运算拓展至三维张量中的基本方法；\n","- 矩阵的线性代数运算：包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等；\n","- 矩阵分解运算：特征分解、奇异值分解和SVD分解等。"]},{"cell_type":"markdown","metadata":{"id":"GTvOiGZ5czO3"},"source":["相关内容如果涉及数学基础，将在讲解过程中逐步补充。"]},{"cell_type":"markdown","metadata":{"id":"20f3eSf-czO4"},"source":["## 二、矩阵的形变及特殊矩阵构造方法"]},{"cell_type":"markdown","metadata":{"id":"u151LlbHczO4"},"source":["&emsp;&emsp;矩阵的形变方法其实也就是二维张量的形变方法，在此基础上本节将补充转置的基本方法。另外，在实际线性代数运算过程中，经常涉及一些特殊矩阵，如单位矩阵、对角矩阵等，相关创建方法如下："]},{"cell_type":"markdown","metadata":{"id":"n8tAYmEhczO5"},"source":["**<center>Tensor矩阵运算</center>**"]},{"cell_type":"markdown","metadata":{"id":"_E5d9GHnczO6"},"source":["|**函数**|**描述**|\n","| :------:| :------: |\n","| torch.t(t)        | t转置| \n","| torch.eye(n)       | 创建包含n个分量的单位矩阵 | \n","| torch.diag(t1)        | 以t1中各元素，创建对角矩阵 | \n","| torch.triu(t)        | 取矩阵t中的上三角矩阵 | \n","| torch.tril(t)        | 取矩阵t中的下三角矩阵 | "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fo-H9OiXczO7","executionInfo":{"status":"ok","timestamp":1648570559562,"user_tz":-120,"elapsed":231,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"72aa5534-3bae-4fac-b634-4940a1fc1102"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])"]},"metadata":{},"execution_count":2}],"source":["# 创建一个2*3的矩阵\n","t1 = torch.arange(1, 7).reshape(2, 3).float()\n","t1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwIc8cV7czO8","executionInfo":{"status":"ok","timestamp":1648570560430,"user_tz":-120,"elapsed":3,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"4e07e833-f696-49bc-fb5c-ed8f488d5a71"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 4.],\n","        [2., 5.],\n","        [3., 6.]])"]},"metadata":{},"execution_count":3}],"source":["# 转置\n","torch.t(t1)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpUQTBsPczO9","executionInfo":{"status":"ok","timestamp":1648570561930,"user_tz":-120,"elapsed":233,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"b5c7c6ef-7df7-4cfd-c2ba-26e8c97ff438"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 4.],\n","        [2., 5.],\n","        [3., 6.]])"]},"metadata":{},"execution_count":4}],"source":["t1.t()"]},{"cell_type":"markdown","metadata":{"id":"hFm7zMZjczO9"},"source":["> 矩阵的转置就是每个元素行列位置互换"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vdoP9toczO9","executionInfo":{"status":"ok","timestamp":1648570563296,"user_tz":-120,"elapsed":252,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"18ae1f2d-1eb3-4b5a-eebc-cad77301611a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])"]},"metadata":{},"execution_count":5}],"source":["torch.eye(3)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mW81wXDgczO-","executionInfo":{"status":"ok","timestamp":1648570582727,"user_tz":-120,"elapsed":234,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"4406c82f-4645-45d3-f2fb-3d47474ce6f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":6}],"source":["t = torch.arange(5)\n","t"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhkWBab0czO-","executionInfo":{"status":"ok","timestamp":1648570587498,"user_tz":-120,"elapsed":219,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"e623c282-5832-4e87-85bb-3794a9151bc4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0],\n","        [0, 1, 0, 0, 0],\n","        [0, 0, 2, 0, 0],\n","        [0, 0, 0, 3, 0],\n","        [0, 0, 0, 0, 4]])"]},"metadata":{},"execution_count":7}],"source":["torch.diag(t)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gI-pQv0SczO-","executionInfo":{"status":"ok","timestamp":1648570594276,"user_tz":-120,"elapsed":370,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"a9e1c503-dde3-45ed-c11e-5bbd1de3c161"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0, 0],\n","        [0, 0, 1, 0, 0, 0],\n","        [0, 0, 0, 2, 0, 0],\n","        [0, 0, 0, 0, 3, 0],\n","        [0, 0, 0, 0, 0, 4],\n","        [0, 0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":8}],"source":["# 对角线向上偏移一位\n","torch.diag(t, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQisUfhSczO_","outputId":"b74298b5-e08c-45b0-e1aa-84969a2c7746"},"outputs":[{"data":{"text/plain":["tensor([[0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0],\n","        [0, 1, 0, 0, 0, 0],\n","        [0, 0, 2, 0, 0, 0],\n","        [0, 0, 0, 3, 0, 0],\n","        [0, 0, 0, 0, 4, 0]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 对角线向下偏移一位\n","torch.diag(t, -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkxPrTM9czPA","outputId":"68f86f46-5a1c-4ef8-ec8d-a542b12bfe91"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["t1 = torch.arange(9).reshape(3, 3)\n","t1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjhVs6awczPA","outputId":"ed14a29e-2b82-435c-f7ff-1892acf88231"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2],\n","        [0, 4, 5],\n","        [0, 0, 8]])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# 取上三角矩阵\n","torch.triu(t1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOVGdfptczPB","outputId":"4553cdd7-2476-4870-983f-b1f9185a7a6b"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [0, 7, 8]])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# 上三角矩阵向左下偏移一位\n","torch.triu(t1, -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIRPHcK2czPB","outputId":"8574ce7c-1636-4739-c22a-077af6931f0a"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2],\n","        [0, 0, 5],\n","        [0, 0, 0]])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# 上三角矩阵向右上偏移一位\n","torch.triu(t1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLI1_kyDczPC","outputId":"d4f8058d-5f9d-4842-a1d5-98e5a98bffab"},"outputs":[{"data":{"text/plain":["tensor([[0, 0, 0],\n","        [3, 4, 0],\n","        [6, 7, 8]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# 下三角矩阵\n","torch.tril(t1)"]},{"cell_type":"markdown","metadata":{"id":"JPddyfmtczPC"},"source":["## 三、矩阵的基本运算"]},{"cell_type":"markdown","metadata":{"id":"33PSNctNczPC"},"source":["&emsp;&emsp;矩阵不同于普通的二维数组，其具备一定的线性代数含义，而这些特殊的性质，其实就主要体现在矩阵的基本运算上。课程中常见的矩阵基本运算如下所示："]},{"cell_type":"markdown","metadata":{"id":"n_OZBAvNczPC"},"source":["**<center>矩阵的基本运算</center>**"]},{"cell_type":"markdown","metadata":{"id":"SVPyMNwZczPC"},"source":["|**函数**|**描述**|\n","| :------:| :------: |\n","| torch.dot(t1, t2)        | 计算t1、t2张量内积 | \n","| torch.mm(t1, t2)        | 矩阵乘法 | \n","| torch.mv(t1, t2)        | 矩阵乘向量 | \n","| torch.bmm(t1, t2)        | 批量矩阵乘法 | \n","| torch.addmm(t, t1, t2)        | 矩阵相乘后相加 | \n","| torch.addbmm(t, t1, t2)        | 批量矩阵相乘后相加 | "]},{"cell_type":"markdown","metadata":{"id":"bOcL_kmjczPD"},"source":["- dot\\vdot：点积计算"]},{"cell_type":"markdown","metadata":{"id":"Xi7_mvhmczPD"},"source":["注意，在PyTorch中，dot和vdot只能作用于一维张量，且对于数值型对象，二者计算结果并没有区别，两种函数只在进行复数运算时会有区别。更多复数运算的规则，我们将在涉及复数运算的场景中再进行详细说明。"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"go-tHdVhczPD","executionInfo":{"status":"ok","timestamp":1648570907830,"user_tz":-120,"elapsed":446,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"11e35926-f278-4c5f-cfcf-0be9200fbbd7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":9}],"source":["t = torch.arange(1, 4)\n","t"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cA1olYTLczPE","executionInfo":{"status":"ok","timestamp":1648570908899,"user_tz":-120,"elapsed":703,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"11ca5511-2327-4e3e-c165-07a90d7b106b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":10}],"source":["torch.dot(t, t)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z98q-KRnczPE","executionInfo":{"status":"ok","timestamp":1648570908900,"user_tz":-120,"elapsed":20,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"ee39fd42-ab91-47d0-ddc6-80dda8790830"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":11}],"source":["torch.vdot(t, t)"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"WGZhlszWczPE","executionInfo":{"status":"error","timestamp":1648570908917,"user_tz":-120,"elapsed":35,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"d1ba34b2-a398-4fbf-976a-704f9863ef1d"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5eafa2b4bbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 不能进行除了一维张量以外的计算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"]}],"source":["# 不能进行除了一维张量以外的计算\n","torch.dot(t1, t1)"]},{"cell_type":"markdown","metadata":{"id":"mvb9yXqYczPF"},"source":["- mm：矩阵乘法"]},{"cell_type":"markdown","metadata":{"id":"Ag_zNuw4czPF"},"source":["&emsp;&emsp;再PyTorch中，矩阵乘法其实是一个函数簇，除了矩阵乘法以外，还有批量矩阵乘法、矩阵相乘相加、批量矩阵相乘相加等等函数。"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfXfUKKHczPF","executionInfo":{"status":"ok","timestamp":1648571044996,"user_tz":-120,"elapsed":213,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"01cc68fe-34f6-4372-f9a9-d92243310b78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":13}],"source":["t1 = torch.arange(1, 7).reshape(2, 3)\n","t1"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qid_WSKCczPF","executionInfo":{"status":"ok","timestamp":1648571048584,"user_tz":-120,"elapsed":767,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"c07a2076-ca1f-4d8f-c453-6eddd47a267d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":14}],"source":["t2 = torch.arange(1, 10).reshape(3, 3)\n","t2"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mHWb046czPF","executionInfo":{"status":"ok","timestamp":1648571065184,"user_tz":-120,"elapsed":332,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"28df0e24-0e29-4209-ce38-81059881bda1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  4,  9],\n","        [16, 25, 36]])"]},"metadata":{},"execution_count":15}],"source":["# 对应位置元素相乘\n","t1 * t1"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZwdzuaMczPG","executionInfo":{"status":"ok","timestamp":1648571066956,"user_tz":-120,"elapsed":250,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"2ce59c69-e05c-487f-8b04-2879dff9dcd6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[30, 36, 42],\n","        [66, 81, 96]])"]},"metadata":{},"execution_count":16}],"source":["# 矩阵乘法\n","torch.mm(t1, t2)"]},{"cell_type":"markdown","metadata":{"id":"QdB_4FcYczPG"},"source":["矩阵乘法执行过程如下所示："]},{"cell_type":"markdown","metadata":{"id":"usw8UD8IczPG"},"source":["![5](https://i.loli.net/2021/01/14/gshVBOWM4QD2TiL.jpg)"]},{"cell_type":"markdown","metadata":{"id":"79kYIGKYczPG"},"source":["- mv：矩阵和向量相乘      \n","&emsp;&emsp;PyTorch中提供了一类非常特殊的矩阵和向量相乘的函数，矩阵和向量相乘的过程我们可以看成是先将向量转化为列向量然后再相乘。"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOd2EY3wczPG","executionInfo":{"status":"ok","timestamp":1648571094763,"user_tz":-120,"elapsed":251,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"70e1802c-9f8b-4e96-842d-8b6c20ed39cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":17}],"source":["met = torch.arange(1, 7).reshape(2, 3)\n","met"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmyswT6PczPH","executionInfo":{"status":"ok","timestamp":1648571096933,"user_tz":-120,"elapsed":247,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"41594fd2-0289-493a-9cb0-8e7edcc58816"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":18}],"source":["vec = torch.arange(1, 4)\n","vec"]},{"cell_type":"markdown","metadata":{"id":"g8ocaVFDczPH"},"source":["在实际执行向量和矩阵相乘的过程中，需要矩阵的列数和向量的元素个数相同"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OgmP13tSczPH","executionInfo":{"status":"ok","timestamp":1648571162790,"user_tz":-120,"elapsed":267,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"ab2fb464-0b6d-41d0-b563-c64f310226bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([14, 32])"]},"metadata":{},"execution_count":19}],"source":["torch.mv(met, vec)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogqPX3kKczPI","executionInfo":{"status":"ok","timestamp":1648571164917,"user_tz":-120,"elapsed":319,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"fea8ec6f-9ea5-43d2-cf95-1b283f30c681"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1],\n","        [2],\n","        [3]])"]},"metadata":{},"execution_count":20}],"source":["vec.reshape(3, 1)             # 转化为列向量"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9LnSBKMczPJ","executionInfo":{"status":"ok","timestamp":1648571167953,"user_tz":-120,"elapsed":235,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"dea69c3d-646c-4398-c8ab-14cad3e6035f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[14],\n","        [32]])"]},"metadata":{},"execution_count":21}],"source":["torch.mm(met, vec.reshape(3, 1))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahXADtIYczPJ","executionInfo":{"status":"ok","timestamp":1648571170797,"user_tz":-120,"elapsed":247,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"7a7ed0a8-0247-4785-a353-3ea126eb2fd1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([14, 32])"]},"metadata":{},"execution_count":22}],"source":["torch.mm(met, vec.reshape(3, 1)).flatten()"]},{"cell_type":"markdown","metadata":{"id":"IyAUVIEJczPJ"},"source":["**理解**：mv函数本质上提供了一种二维张量和一维张量相乘的方法，在线性代数运算过程中，有很多矩阵乘向量的场景，典型的如线性回归的求解过程，通常情况下我们需要将向量转化为列向量（或者某些编程语言就默认向量都是列向量）然后进行计算，但PyTorch中单独设置了一个矩阵和向量相乘的方法，从而简化了行/列向量的理解过程和将向量转化为列向量的转化过程。"]},{"cell_type":"markdown","metadata":{"id":"JmwPfbc8czPJ"},"source":["- bmm：批量矩阵相乘"]},{"cell_type":"markdown","metadata":{"id":"gh7FcTJfczPK"},"source":["&emsp;&emsp;所谓批量矩阵相乘，指的是三维张量的矩阵乘法。根据此前对张量结构的理解，我们知道，三维张量就是一个包含了多个相同形状的矩阵的集合。例如，一个（3， 2， 2）的张量，本质上就是一个包含了3个2*2矩阵的张量。而三维张量的矩阵相乘，则是三维张量内部各对应位置的矩阵相乘。由于张量的运算往往涉及二维及以上，因此批量矩阵相乘也有非常多的应用场景。"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLOVheWQczPK","executionInfo":{"status":"ok","timestamp":1648571368491,"user_tz":-120,"elapsed":244,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"f9822386-5e8b-4430-a3c7-71d37c9af637"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1,  2],\n","         [ 3,  4]],\n","\n","        [[ 5,  6],\n","         [ 7,  8]],\n","\n","        [[ 9, 10],\n","         [11, 12]]])"]},"metadata":{},"execution_count":23}],"source":["t3 = torch.arange(1, 13).reshape(3, 2, 2)\n","t3"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V52AC9QUczPL","executionInfo":{"status":"ok","timestamp":1648571370680,"user_tz":-120,"elapsed":221,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"243b34e9-f0cc-45bb-eefa-71832e459b49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1,  2,  3],\n","         [ 4,  5,  6]],\n","\n","        [[ 7,  8,  9],\n","         [10, 11, 12]],\n","\n","        [[13, 14, 15],\n","         [16, 17, 18]]])"]},"metadata":{},"execution_count":24}],"source":["t4 = torch.arange(1, 19).reshape(3, 2, 3)\n","t4"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAsmVKD9czPL","executionInfo":{"status":"ok","timestamp":1648571373035,"user_tz":-120,"elapsed":8,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"0994201f-f3ff-4cbe-97ff-da655f87e60b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[  9,  12,  15],\n","         [ 19,  26,  33]],\n","\n","        [[ 95, 106, 117],\n","         [129, 144, 159]],\n","\n","        [[277, 296, 315],\n","         [335, 358, 381]]])"]},"metadata":{},"execution_count":25}],"source":["torch.bmm(t3, t4)"]},{"cell_type":"markdown","metadata":{"id":"OQbTRfZDczPL"},"source":["**Point:**     \n","- 三维张量包含的矩阵个数需要相同；\n","- 每个内部矩阵，需要满足矩阵乘法的条件，也就是左乘矩阵的行数要等于右乘矩阵的列数。"]},{"cell_type":"markdown","metadata":{"id":"OKUA2LArczPM"},"source":["- addmm：矩阵相乘后相加"]},{"cell_type":"markdown","metadata":{"id":"ladrkIduczPM"},"source":["addmm函数结构：addmm(input, mat1, mat2, beta=1, alpha=1)       \n","输出结果：beta * input + alpha * (mat1 * mat2)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhDXCPpEczPM","executionInfo":{"status":"ok","timestamp":1648571411925,"user_tz":-120,"elapsed":312,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"ee843c07-1837-4d77-9a0a-34220be496f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":26}],"source":["t1"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4IOKs59czPM","executionInfo":{"status":"ok","timestamp":1648571413618,"user_tz":-120,"elapsed":280,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"03941a81-75a4-4244-e8df-4ce42f6c43a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":27}],"source":["t2"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxZ9vzAaczPM","executionInfo":{"status":"ok","timestamp":1648571415551,"user_tz":-120,"elapsed":230,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"ae36ac0f-597d-4d5c-e37b-05afdf3ae2d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2])"]},"metadata":{},"execution_count":28}],"source":["t = torch.arange(3)\n","t"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TahF9t1rczPM","executionInfo":{"status":"ok","timestamp":1648571417505,"user_tz":-120,"elapsed":236,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"eb63c357-f0f9-45b9-f7e4-56ce13f39e3b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[30, 36, 42],\n","        [66, 81, 96]])"]},"metadata":{},"execution_count":29}],"source":["torch.mm(t1, t2)                    # 矩阵乘法"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uKBEK79czPN","executionInfo":{"status":"ok","timestamp":1648571433657,"user_tz":-120,"elapsed":277,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"89fc20e6-9c33-4982-95d2-8a8f750ea888"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[30, 37, 44],\n","        [66, 82, 98]])"]},"metadata":{},"execution_count":30}],"source":["torch.addmm(t, t1, t2)              # 先乘法后相加   "]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHSg70LzczPN","executionInfo":{"status":"ok","timestamp":1648571435259,"user_tz":-120,"elapsed":5,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"51dd1d51-5c28-435c-8bed-c359b41e43b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[300, 360, 420],\n","        [660, 810, 960]])"]},"metadata":{},"execution_count":31}],"source":["torch.addmm(t, t1, t2, beta = 0, alpha = 10)"]},{"cell_type":"markdown","metadata":{"id":"WpptwkRGczPN"},"source":["- addbmm：批量矩阵相乘后相加"]},{"cell_type":"markdown","metadata":{"id":"RmKrebxnczPN"},"source":["&emsp;&emsp;和addmm类似，都是先乘后加，并且可以设置权重。不同的是addbmm是批量矩阵相乘，并且，在相加的过程中也是矩阵相加，而非向量加矩阵。"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5PqWq1wczPN","executionInfo":{"status":"ok","timestamp":1648571843765,"user_tz":-120,"elapsed":732,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"3a6f1781-8ada-4152-8b67-57edbc02979d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1, 2],\n","        [3, 4, 5]])"]},"metadata":{},"execution_count":32}],"source":["t = torch.arange(6).reshape(2, 3)\n","t"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vyIYWrc3czPN","executionInfo":{"status":"ok","timestamp":1648571845479,"user_tz":-120,"elapsed":3,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"93316848-a68a-43ee-8b9b-98eb6ac272bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1,  2],\n","         [ 3,  4]],\n","\n","        [[ 5,  6],\n","         [ 7,  8]],\n","\n","        [[ 9, 10],\n","         [11, 12]]])"]},"metadata":{},"execution_count":33}],"source":["t3"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFNnM5GTczPN","executionInfo":{"status":"ok","timestamp":1648571848658,"user_tz":-120,"elapsed":567,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"8480a82f-3ca2-4342-b65f-1a54b9338b91"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1,  2,  3],\n","         [ 4,  5,  6]],\n","\n","        [[ 7,  8,  9],\n","         [10, 11, 12]],\n","\n","        [[13, 14, 15],\n","         [16, 17, 18]]])"]},"metadata":{},"execution_count":34}],"source":["t4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4QGmWhBczPO","executionInfo":{"status":"aborted","timestamp":1648570908917,"user_tz":-120,"elapsed":28,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["torch.bmm(t3, t4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64OloxX2czPO","executionInfo":{"status":"aborted","timestamp":1648570908917,"user_tz":-120,"elapsed":28,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["torch.addbmm(t, t3, t4)"]},{"cell_type":"markdown","metadata":{"id":"9wWWVIvnczPO"},"source":["**注：**addbmm会在原来三维张量基础之上，对其内部矩阵进行求和"]},{"cell_type":"markdown","metadata":{"id":"cEisGKF6czPO"},"source":["## 四、矩阵的线性代数运算"]},{"cell_type":"markdown","metadata":{"id":"Dc0SNxbsczPO"},"source":["&emsp;&emsp;如果说矩阵的基本运算是矩阵基本性质，那么矩阵的线性代数运算，则是我们利用矩阵数据类型在求解实际问题过程中经常涉及到的线性代数方法，具体相关函数如下："]},{"cell_type":"markdown","metadata":{"id":"PmiEH883czPO"},"source":["**<center>矩阵的线性代数运算</center>**"]},{"cell_type":"markdown","metadata":{"id":"Jnnok7FeczPO"},"source":["|**函数**|**描述**|\n","| :------:| :------: |\n","| torch.trace(A)       | 矩阵的迹 |\n","| matrix_rank(A)       | 矩阵的秩 |\n","| torch.det(A)         | 计算矩阵A的行列式 |  \n","| torch.inverse(A)        | 矩阵求逆 | \n","| torch.lstsq(A,B)        | 最小二乘法 | "]},{"cell_type":"markdown","metadata":{"id":"Chg9ykBAczPO"},"source":["同时，由于线性代数所涉及的数学基础知识较多，从实际应用的角度出发，我们将有所侧重的介绍实际应用过程中需要掌握的相关内容，并通过本节末尾的实际案例，来加深线性代数相关内容的理解。"]},{"cell_type":"markdown","metadata":{"id":"6ENsX654czPP"},"source":["### 1.矩阵的迹（trace）"]},{"cell_type":"markdown","metadata":{"id":"UiGquW8FczPP"},"source":["&emsp;&emsp;矩阵的迹的运算相对简单，就是矩阵对角线元素之和，在PyTorch中，可以使用trace函数进行计算。"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpwLJTVNczPP","executionInfo":{"status":"ok","timestamp":1648571852289,"user_tz":-120,"elapsed":543,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"a5aa7474-a1d7-4449-b853-134037c211b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [4., 5.]])"]},"metadata":{},"execution_count":35}],"source":["A = torch.tensor([[1, 2], [4, 5]]).float()  \n","A"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxqLQ6f6czPP","executionInfo":{"status":"ok","timestamp":1648571853267,"user_tz":-120,"elapsed":722,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"7cfdd092-2db9-478e-db9b-fb2ba7a3ac4f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.)"]},"metadata":{},"execution_count":36}],"source":["torch.trace(A)"]},{"cell_type":"markdown","metadata":{"id":"spI7GbXLczPP"},"source":["当然，对于矩阵的迹来说，计算过程不需要是方阵"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNAWGUBHczPQ","executionInfo":{"status":"ok","timestamp":1648571853269,"user_tz":-120,"elapsed":37,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"0f0356d6-fda6-44fd-c8ab-8cf0c39e78a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":37}],"source":["B = torch.arange(1, 7).reshape(2, 3)\n","B"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DrUUJXsoczPQ","executionInfo":{"status":"ok","timestamp":1648571853270,"user_tz":-120,"elapsed":37,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"c9ade1be-23e6-4f67-8cb8-3931cf7224f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6)"]},"metadata":{},"execution_count":38}],"source":["torch.trace(B)"]},{"cell_type":"markdown","metadata":{"id":"0JHqe7AfczPQ"},"source":["### 2.矩阵的秩(rank)"]},{"cell_type":"markdown","metadata":{"id":"vmwRNhSDczPR"},"source":["&emsp;&emsp;矩阵的秩（rank），是指矩阵中行或列的极大线性无关数，且矩阵中行、列极大无关数总是相同的，任何矩阵的秩都是唯一值，满秩指的是方阵（行数和列数相同的矩阵）中行数、列数和秩相同，满秩矩阵有线性唯一解等重要特性，而其他矩阵也能通过求解秩来降维，同时，秩也是奇异值分解等运算中涉及到的重要概念。"]},{"cell_type":"markdown","metadata":{"id":"lICp7ePWczPR"},"source":["- matrix_rank计算矩阵的秩"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IC5Tz8gczPR","executionInfo":{"status":"ok","timestamp":1648571853271,"user_tz":-120,"elapsed":36,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"ac69a7de-d4e1-492c-ff42-a5c7dd77cffa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{},"execution_count":39}],"source":["A = torch.arange(1, 5).reshape(2, 2).float()\n","A"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6pvKmKiczPS","executionInfo":{"status":"ok","timestamp":1648571853272,"user_tz":-120,"elapsed":34,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"72e66162-4a03-4930-9ae7-b5e4596aaf94"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  ../aten/src/ATen/native/LinearAlgebra.cpp:473.)\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(2)"]},"metadata":{},"execution_count":40}],"source":["torch.matrix_rank(A)"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCx5lzbRczPS","executionInfo":{"status":"ok","timestamp":1648571853273,"user_tz":-120,"elapsed":33,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"062be841-ab9e-457a-be46-7b327ceef542"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [2., 4.]])"]},"metadata":{},"execution_count":41}],"source":["B = torch.tensor([[1, 2], [2, 4]]).float()\n","B"]},{"cell_type":"markdown","metadata":{"id":"C1yAr4BWczPS"},"source":["对于矩阵B来说，第一列和第二列明显线性相关，最大线性无关组只有1组，因此矩阵的秩计算结果为1"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMSsbzqZczPT","executionInfo":{"status":"ok","timestamp":1648571853274,"user_tz":-120,"elapsed":33,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"a62df2e3-2578-4c7c-8382-a33327371118"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1)"]},"metadata":{},"execution_count":42}],"source":["torch.matrix_rank(B)"]},{"cell_type":"markdown","metadata":{"id":"aV7PFJ4XczPT"},"source":["### 3.矩阵的行列式(det)"]},{"cell_type":"markdown","metadata":{"id":"sUPUZSCFczPY"},"source":["&emsp;&emsp;所谓行列式，我们可以简单将其理解为矩阵的一个基本性质或者属性，通过行列式的计算，我们能够知道矩阵是否可逆，从而可以进一步求解矩阵所对应的线性方程。当然，更加专业的解释，行列式的作为一个基本数学工具，实际上就是矩阵进行线性变换的伸缩因子。"]},{"cell_type":"markdown","metadata":{"id":"70ndIqcBczPY"},"source":["对于任何一个n维方正，行列式计算过程如下："]},{"cell_type":"markdown","metadata":{"id":"uY8YZrATczPZ"},"source":["<img src=\"https://i.loli.net/2021/01/14/AkeTpgOrctHoIiq.jpg\" alt=\"7\" style=\"zoom:50%;\" />"]},{"cell_type":"markdown","metadata":{"id":"ys9FlfYRczPZ"},"source":["更为简单的情况，如果对于一个2*2的矩阵，行列式的计算就是主对角线元素之积减去另外两个元素之积"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZ4rk3g0czPZ","executionInfo":{"status":"ok","timestamp":1648572118732,"user_tz":-120,"elapsed":252,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"5dbb804e-d0a1-48da-b35a-55711a60368c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [4., 5.]])"]},"metadata":{},"execution_count":49}],"source":["A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n","A"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"235ncgpNczPZ","executionInfo":{"status":"ok","timestamp":1648571853276,"user_tz":-120,"elapsed":32,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"e019aac8-aa8a-4020-afb1-96a5d2ba892a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-3.)"]},"metadata":{},"execution_count":44}],"source":["torch.det(A)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b0wYuv_czPZ","executionInfo":{"status":"ok","timestamp":1648571853277,"user_tz":-120,"elapsed":32,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"dee844d3-f952-4b45-e1db-142e8e47b197"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [2., 4.]])"]},"metadata":{},"execution_count":45}],"source":["B"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8pFhZmRCczPZ","executionInfo":{"status":"ok","timestamp":1648571853279,"user_tz":-120,"elapsed":33,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"e013e9cb-d1db-450a-f264-b9ef25bab4e8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.)"]},"metadata":{},"execution_count":46}],"source":["torch.det(B)"]},{"cell_type":"markdown","metadata":{"id":"4VpJzijDczPZ"},"source":["A的行列式计算过程如下："]},{"cell_type":"markdown","metadata":{"id":"oWNePIrDczPa"},"source":["<img src=\"https://i.loli.net/2021/01/15/wMvxGTu7a2VCzE9.jpg\" alt=\"6\" style=\"zoom:50%;\" />"]},{"cell_type":"markdown","metadata":{"id":"8FOR1JkNczPa"},"source":["对于行列式的计算，要求二维张量必须是方正，也就是行列数必须一致。"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSrRSQkMczPa","executionInfo":{"status":"ok","timestamp":1648571853279,"user_tz":-120,"elapsed":32,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"01d74696-2ed3-4235-c69f-5500b6b10298"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":47}],"source":["B = torch.arange(1, 7).reshape(2, 3)\n","B"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"PnySAQXVczPa","executionInfo":{"status":"error","timestamp":1648571853284,"user_tz":-120,"elapsed":36,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"fb99ff34-8cd1-4588-d288-35a0bec74a1e"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-beff1455abd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: A must be batches of square matrices, but they are 3 by 2 matrices"]}],"source":["torch.det(B)"]},{"cell_type":"markdown","metadata":{"id":"3i-hYvkyczPa"},"source":["### 3.线性方程组的矩阵表达形式"]},{"cell_type":"markdown","metadata":{"id":"Qi7_TxiaczPa"},"source":["&emsp;&emsp;在正式进入到更进一步矩阵运算的讨论之前，我们需要对矩阵建立一个更加形象化的理解。通常来说，我们会把高维空间中的一个个数看成是向量，而由这些向量组成的数组看成是一个矩阵。例如：（1，2），（3，4）是二维空间中的两个点，矩阵A就代表这两个点所组成的矩阵。"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mu-iEM-eczPa","executionInfo":{"status":"ok","timestamp":1648572193912,"user_tz":-120,"elapsed":272,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"3f44a661-0080-45f6-8d30-0c000a4465bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{},"execution_count":50}],"source":["A = torch.arange(1, 5).reshape(2, 2).float()\n","A"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"u7Lq8tR2czPb","executionInfo":{"status":"ok","timestamp":1648572196563,"user_tz":-120,"elapsed":306,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"vNX_4fwJczPb","executionInfo":{"status":"ok","timestamp":1648572197984,"user_tz":-120,"elapsed":391,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"b5a897fc-992f-41d8-a874-05d65ade0940"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fe9009236d0>]"]},"metadata":{},"execution_count":52},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAV8klEQVR4nO3dcYyc9X3n8fenyxYciLCJNym1DeZalEuAgHMjJ61RG0jBTpoAzUU6pylHT0SWcuQOehGnwElEoX+EHlKSu+pyxAqoTo4EXDDUR0PAOlyllLPJrDExtnHrAxrYInmDMeDGcrPO5/6Yx814mdl9xh7Pen98XtLIz/ye3zPPdx5+fHb2eZ7Zn2wTERHl+qWZLiAiIo6vBH1EROES9BERhUvQR0QULkEfEVG4k2a6gE7mz5/vxYsXz3QZERGzxujo6E9sj3Rad0IG/eLFi2k2mzNdRkTErCHp77uty6mbiIjCJegjIgqXoI+IKFyCPiKicAn6iIjC1Q56SUOSnpL0UId1J0u6V9JuSZslLW5bd1PVvkvS8v6UHRFRjgefGmPZbY9xzhf+kmW3PcaDT4319fV7+UR/PbCzy7prgVdt/zrwVeBPACS9F1gJnAesAL4uaejoy42IKMuDT41x07ptjO07gIGxfQe4ad22voZ9raCXtBD4XeCbXbpcCayplu8DPixJVfs9tg/afh7YDSw9tpIjIspx+yO7OPCzQ0e0HfjZIW5/ZFff9lH3E/3XgP8M/LzL+gXAiwC2J4DXgHe0t1deqtreRNIqSU1JzfHx8ZplRUTMbv+w70BP7Udj2qCX9DFgj+3Rvu21A9urbTdsN0ZGOn6LNyKiOL86d05P7Uejzif6ZcAVkl4A7gEulfS/JvUZAxYBSDoJOB14pb29srBqi4gI4Mbl72bO8JGXLucMD3Hj8nf3bR/TBr3tm2wvtL2Y1oXVx2z/waRu64FrquVPVn1cta+s7so5BzgXeLJv1UdEzHJXLVnAlz9xAQvmzkHAgrlz+PInLuCqJR3Pch+Vo/6jZpJuBZq21wN3At+WtBvYS+sHAra3S1oL7AAmgOtsH+r2mhERb0VXLVnQ12CfTCfi5OCNRsP565UREfVJGrXd6LQu34yNiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKN+0MU5JOAX4AnFz1v8/2Fyf1+SpwSfX0bcA7bc+t1h0CtlXrfmz7ij7VHhERNdSZSvAgcKnt/ZKGgcclPWx70+EOtv/o8LKk/wAsadv+gO2L+lZxRET0pM7k4La9v3o6XD2mmn/wU8B3+1BbRET0Qa1z9JKGJG0F9gAbbG/u0u9s4BzgsbbmUyQ1JW2SdNUU+1hV9WuOj4/38BYiImIqtYLe9qHq9MtCYKmk87t0XUnrHP6htrazqwlrfx/4mqRf67KP1bYbthsjIyM9vIWIiJhKT3fd2N4HbARWdOmykkmnbWyPVf8+B/wVR56/j4iI42zaoJc0IunwHTRzgMuAZzv0+5fAPOD/trXNk3RytTwfWAbs6E/pERFRR527bs4E1kgaovWDYa3thyTdCjRtr6/6rQTusd1+ofY9wDck/bza9jbbCfqIiAHSkbl8Ymg0Gm42mzNdRkTErCFptLoe+ib5ZmxEROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QULkEfEVG4OlMJniLpSUlPS9ou6Usd+vyhpHFJW6vHZ9rWXSPp76rHNf1+AxERMbU6UwkeBC61vV/SMPC4pIdtb5rU717bn2tvkHQG8EWgARgYlbTe9qv9KD4iIqY37Sd6t+yvng5Xj7rzDy4HNtjeW4X7BmDFUVUaERFHpdY5eklDkrYCe2gF9+YO3f61pB9Juk/SoqptAfBiW5+XqrZO+1glqSmpOT4+3sNbiIiIqdQKetuHbF8ELASWSjp/Upf/DSy2/T5an9rX9FqI7dW2G7YbIyMjvW4eERFd9HTXje19wEYmnX6x/Yrtg9XTbwL/qloeAxa1dV1YtUVExIDUuetmRNLcankOcBnw7KQ+Z7Y9vQLYWS0/AlwuaZ6kecDlVVtERAxInbtuzgTWSBqi9YNhre2HJN0KNG2vB/6jpCuACWAv8IcAtvdK+mPgh9Vr3Wp7b7/fREREdCe77g00g9NoNNxsNme6jIiIWUPSqO1Gp3X5ZmxEROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QULkEfEVG4BH1EROES9BERhUvQR0QUrs4MU6dIelLS05K2S/pShz7/SdKOanLw/yPp7LZ1hyRtrR7r+/0GIiJianVmmDoIXGp7v6Rh4HFJD9ve1NbnKaBh+6eSPgv8V+DfVOsOVBOLR0TEDJj2E71b9ldPh6uHJ/XZaPun1dNNtCYBj4iIE0Ctc/SShiRtBfYAG2xvnqL7tcDDbc9PkdSUtEnSVVPsY1XVrzk+Pl6r+IiImF6toLd9qDr9shBYKun8Tv0k/QHQAG5vaz67msfw94GvSfq1LvtYbbthuzEyMtLTm4iIiO56uuvG9j5gI7Bi8jpJvwP8F+AK2wfbthmr/n0O+CtgyTHUGxERPapz182IpLnV8hzgMuDZSX2WAN+gFfJ72trnSTq5Wp4PLAN29K/8iIiYTp27bs4E1kgaovWDYa3thyTdCjRtr6d1quY04M8lAfzY9hXAe4BvSPp5te1tthP0EREDNG3Q2/4RHU632L6lbfl3umz7BHDBsRQYERHHJt+MjYgoXII+IqJwCfqIiMIl6CMiCpegj4goXII+IqJwCfqIiMIl6CMiCpegj4goXII+IqJwCfqIiMIl6CMiCpegj4goXII+IqJwCfqIiMIl6CMiCldnKsFTJD0p6WlJ2yV9qUOfkyXdK2m3pM2SFretu6lq3yVpeX/Lj4iI6dT5RH8QuNT2hcBFwApJH5zU51rgVdu/DnwV+BMASe8FVgLn0ZpQ/OvVlIQRETEg0wa9W/ZXT4erhyd1uxJYUy3fB3xYrcljrwTusX3Q9vPAbmBpXyqPiIhaap2jlzQkaSuwB9hge/OkLguAFwFsTwCvAe9ob6+8VLV12scqSU1JzfHx8d7eRUREdFUr6G0fsn0RsBBYKun8fhdie7Xthu3GyMhIv18+IuItq6e7bmzvAzbSOt/ebgxYBCDpJOB04JX29srCqi0iIgakzl03I5LmVstzgMuAZyd1Ww9cUy1/EnjMtqv2ldVdOecA5wJP9qv4iIiY3kk1+pwJrKnulvklYK3thyTdCjRtrwfuBL4taTewl9adNtjeLmktsAOYAK6zfeh4vJGIiOhMrQ/eJ5ZGo+FmsznTZUREzBqSRm03Oq3LN2MjIgqXoI+IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCTTvDlKRFwLeAdwEGVtv+b5P63Ah8uu013wOM2N4r6QXgDeAQMNHtD+NHRMTxUWcqwQng87a3SHo7MCppg+0dhzvYvh24HUDSx4E/sr237TUusf2TfhYeERH1THvqxvbLtrdUy28AO4EFU2zyKeC7/SkvIiKOVU/n6CUtBpYAm7usfxuwAri/rdnAo5JGJa2a4rVXSWpKao6Pj/dSVkRETKF20Es6jVaA32D79S7dPg78zaTTNhfbfj/wEeA6Sb/VaUPbq203bDdGRkbqlhUREdOoFfSShmmF/N22103RdSWTTtvYHqv+3QM8ACw9ulIjIuJoTBv0kgTcCey0/ZUp+p0O/DbwF21tp1YXcJF0KnA58MyxFh0REfXVuetmGXA1sE3S1qrtZuAsANt3VG2/Bzxq+x/btn0X8EDrZwUnAd+x/f1+FB4REfVMG/S2HwdUo9+fAX82qe054MKjrC0iIvog34yNiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwtWZYWqRpI2SdkjaLun6Dn0+JOk1SVurxy1t61ZI2iVpt6Qv9PsNRETE1OrMMDUBfN72lmpawFFJG2zvmNTvr21/rL1B0hDwP4DLgJeAH0pa32HbiIg4Tqb9RG/7ZdtbquU3gJ3AgpqvvxTYbfs52/8E3ANcebTFRkRE73o6Ry9pMbAE2Nxh9W9IelrSw5LOq9oWAC+29XmJLj8kJK2S1JTUHB8f76WsiIiYQu2gl3QacD9wg+3XJ63eApxt+0LgT4EHey3E9mrbDduNkZGRXjePiIguagW9pGFaIX+37XWT19t+3fb+avl7wLCk+cAYsKit68KqLSIiBqTOXTcC7gR22v5Klz6/UvVD0tLqdV8BfgicK+kcSb8MrATW96v4iIiYXp27bpYBVwPbJG2t2m4GzgKwfQfwSeCzkiaAA8BK2wYmJH0OeAQYAu6yvb3P7yEiIqagVh6fWBqNhpvN5kyXERExa0gatd3otC7fjI2IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCJegjIgqXoI+IKFyCPiKicAn6iIjCJegjIgpXZyrBRZI2Stohabuk6zv0+bSkH0naJukJSRe2rXuhat8qKbOJREQMWJ2pBCeAz9veIuntwKikDbZ3tPV5Hvht269K+giwGvhA2/pLbP+kf2VHRERd0wa97ZeBl6vlNyTtBBYAO9r6PNG2ySZgYZ/rjIiIo9TTOXpJi4ElwOYpul0LPNz23MCjkkYlrZritVdJakpqjo+P91JWRERMoc6pGwAknQbcD9xg+/UufS6hFfQXtzVfbHtM0juBDZKetf2DydvaXk3rlA+NRuPEm7E8ImKWqvWJXtIwrZC/2/a6Ln3eB3wTuNL2K4fbbY9V/+4BHgCWHmvRERFRX527bgTcCey0/ZUufc4C1gFX2/7btvZTqwu4SDoVuBx4ph+FR0REPXVO3SwDrga2Sdpatd0MnAVg+w7gFuAdwNdbPxeYsN0A3gU8ULWdBHzH9vf7+g4iImJKde66eRzQNH0+A3ymQ/tzwIVv3iIiIgYl34yNiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKl6CPiChcgj4ionAJ+oiIwiXoIyIKV2cqwUWSNkraIWm7pOs79JGk/y5pt6QfSXp/27prJP1d9bim32/gsAefGmPZbY9xzhf+kmW3PcaDT40dr11FRMwqdaYSnAA+b3tLNf/rqKQNtne09fkIcG71+ADwP4EPSDoD+CLQAFxtu972q/18Ew8+NcZN67Zx4GeHABjbd4Cb1m0D4KolC/q5q4iIWWfaT/S2X7a9pVp+A9gJTE7PK4FvuWUTMFfSmcByYIPtvVW4bwBW9PUdALc/suufQ/6wAz87xO2P7Or3riIiZp2eztFLWgwsATZPWrUAeLHt+UtVW7f2Tq+9SlJTUnN8fLyXsviHfQd6ao+IeCupHfSSTgPuB26w/Xq/C7G92nbDdmNkZKSnbX917pye2iMi3kpqBb2kYVohf7ftdR26jAGL2p4vrNq6tffVjcvfzZzhoSPa5gwPcePyd/d7VxERs06du24E3AnstP2VLt3WA/+2uvvmg8Brtl8GHgEulzRP0jzg8qqtr65asoAvf+ICFsydg4AFc+fw5U9ckAuxERHUu+tmGXA1sE3S1qrtZuAsANt3AN8DPgrsBn4K/Ltq3V5Jfwz8sNruVtt7+1f+L1y1ZEGCPSKig2mD3vbjgKbpY+C6LuvuAu46quoiIuKY5ZuxERGFS9BHRBQuQR8RUbgEfURE4dS6jnpikTQO/P1Rbj4f+Ekfy+mX1NWb1NWb1NWbEus623bHb5uekEF/LCQ1bTdmuo7JUldvUldvUldv3mp15dRNREThEvQREYUrMehXz3QBXaSu3qSu3qSu3ryl6iruHH1ERBypxE/0ERHRJkEfEVG4WRP0ku6StEfSM13Wz8gE5TXq+nRVzzZJT0i6sG3dC1X7VknNAdf1IUmvVfveKumWtnUrJO2qjuUXBlzXjW01PSPpUDX38PE+XoskbZS0Q9J2Sdd36DPwMVazroGPsZp1DXyM1axr4GNM0imSnpT0dFXXlzr0OVnSvdUx2azWjH6H191Ute+StLznAmzPigfwW8D7gWe6rP8o8DCtv7T5QWBz1X4G8Fz177xqed4A6/rNw/ujNYn65rZ1LwDzZ+h4fQh4qEP7EPD/gH8B/DLwNPDeQdU1qe/HgccGdLzOBN5fLb8d+NvJ73smxljNugY+xmrWNfAxVqeumRhj1Zg5rVoepjUd6wcn9fn3wB3V8krg3mr5vdUxOhk4pzp2Q73sf9Z8orf9A2Cqv2U/IxOUT1eX7Seq/QJsojXL1nFX43h1sxTYbfs52/8E3EPr2M5EXZ8CvtuvfU/F9su2t1TLbwA7efP8xgMfY3XqmokxVvN4dXPcxthR1DWQMVaNmf3V0+HqMflOmCuBNdXyfcCHJalqv8f2QdvP05r3Y2kv+581QV/DMU9QPgDX0vpEeJiBRyWNSlo1A/X8RvWr5MOSzqvaTojjJelttMLy/rbmgRyv6lfmJbQ+dbWb0TE2RV3tBj7GpqlrxsbYdMdr0GNM0pBakzftofXBoOv4sj0BvAa8gz4crzozTEUfSLqE1v+EF7c1X2x7TNI7gQ2Snq0+8Q7CFlp/G2O/pI8CDwLnDmjfdXwc+BsfOSPZcT9ekk6j9T/+DbZf7+drH4s6dc3EGJumrhkbYzX/Ow50jNk+BFwkaS7wgKTzbXe8VtVvJX2in9EJyqci6X3AN4Erbb9yuN32WPXvHuABevx17FjYfv3wr5K2vwcMS5rPCXC8KiuZ9Cv18T5ekoZphcPdttd16DIjY6xGXTMyxqara6bGWJ3jVRn4GKteex+wkTef3vvn4yLpJOB04BX6cbz6fdHheD6AxXS/uPi7HHmh7Mmq/QzgeVoXyeZVy2cMsK6zaJ1T+81J7acCb29bfgJYMcC6foVffGFuKfDj6tidROti4jn84kLZeYOqq1p/Oq3z+KcO6nhV7/1bwNem6DPwMVazroGPsZp1DXyM1alrJsYYMALMrZbnAH8NfGxSn+s48mLs2mr5PI68GPscPV6MnTWnbiR9l9ZV/PmSXgK+SOuCBp7BCcpr1HULrfNsX29dV2HCrb9O9y5av75Ba+B/x/b3B1jXJ4HPSpoADgAr3RpVE5I+BzxC6+6Iu2xvH2BdAL8HPGr7H9s2Pa7HC1gGXA1sq86jAtxMK0RncozVqWsmxlidumZijNWpCwY/xs4E1kgaonUmZa3thyTdCjRtrwfuBL4taTetH0Irq5q3S1oL7AAmgOvcOg1UW/4EQkRE4Uo6Rx8RER0k6CMiCpegj4goXII+IqJwCfqIiMIl6CMiCpegj4go3P8H52ix4968vjYAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}],"source":["# 绘制点图查看两个点的位置\n","plt.plot(A[:,0], A[:, 1], 'o')"]},{"cell_type":"markdown","metadata":{"id":"HC0eORXRczPb"},"source":["如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是所谓的构建一个线性回归模型，我们可以设置线性回归方程如下："]},{"cell_type":"markdown","metadata":{"id":"edOZOE3PczPb"},"source":["<center> $ y = ax + b $ </center>"]},{"cell_type":"markdown","metadata":{"id":"7hwjxyV8czPb"},"source":["带入（1，2）和（3，4）两个点之后，我们还可以进一步将表达式改写成矩阵表示形式，改写过程如下"]},{"cell_type":"markdown","metadata":{"id":"EMhKEcB5czPb"},"source":["<img src=\"https://i.loli.net/2021/01/14/UEPNYc9OjGn3J5b.jpg\" alt=\"8\" style=\"zoom:50%;\" />"]},{"cell_type":"markdown","metadata":{"id":"x9b-cQ6SczPb"},"source":["而用矩阵表示线性方程组，则是矩阵的另一种常用用途，接下来，我们就可以通过上述矩阵方程组来求解系数向量x。"]},{"cell_type":"markdown","metadata":{"id":"EFtfCkj9czPb"},"source":["&emsp;&emsp;首先一个基本思路是，如果有个和A矩阵相关的另一个矩阵，假设为$A^{-1}$，可以使得二者相乘之后等于1，也就是$A * A^{-1} = 1$，那么在方程组左右两边同时左乘该矩阵，等式右边的计算结果$A^{-1} * B$就将是x系数向量的取值。而此处的$A^{-1}$就是所谓的A的逆矩阵。"]},{"cell_type":"markdown","metadata":{"id":"nLygs506czPc"},"source":["逆矩阵定义：    \n","<center> $ 如果存在两个矩阵A、B，并在矩阵乘法运算下，A * B = E（单位矩阵），则我们称A、B互为逆矩阵$  </center>"]},{"cell_type":"markdown","metadata":{"id":"JOvQhbFMczPc"},"source":["在上述线性方程组求解场景中，我们已经初步看到了逆矩阵的用途，而一般来说，我们往往会通过伴随矩阵来进行逆矩阵的求解。由于伴随矩阵本身并无其他核心用途，且PyTorch中也未给出伴随矩阵的计算函数（目前），因此我们直接调用inverse函数来进行逆矩阵的计算。"]},{"cell_type":"markdown","metadata":{"id":"p2aa8KdDczPc"},"source":["> 当然，并非所有矩阵都有逆矩阵，对于一个矩阵来说，首先必须是方正，其次矩阵的秩不能为零，满足两个条件才能求解逆矩阵。"]},{"cell_type":"markdown","metadata":{"id":"2pbgGd6rczPc"},"source":["- inverse函数：求解逆矩阵"]},{"cell_type":"markdown","metadata":{"id":"xbk7biNwczPc"},"source":["首先，根据上述矩阵表达式，从新定义A和B"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbM7ZmykczPc","executionInfo":{"status":"ok","timestamp":1648572399871,"user_tz":-120,"elapsed":250,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"28ef91da-81c0-4334-f084-06eb333b534a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1.],\n","        [3., 1.]])"]},"metadata":{},"execution_count":53}],"source":["A = torch.tensor([[1.0, 1], [3, 1]])\n","A"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQuiQ1nYczPd","executionInfo":{"status":"ok","timestamp":1648572403772,"user_tz":-120,"elapsed":304,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"9714d487-6221-452a-b0fb-f4cecb6b70bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 4.])"]},"metadata":{},"execution_count":54}],"source":["B = torch.tensor([2.0, 4])\n","B"]},{"cell_type":"markdown","metadata":{"id":"1gItkeEAczPd"},"source":["然后使用inverse函数进行逆矩阵求解"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxdv8z6hczPd","executionInfo":{"status":"ok","timestamp":1648572411254,"user_tz":-120,"elapsed":277,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"29bcf42c-cebe-4526-978a-0baa22037ddb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.5000,  0.5000],\n","        [ 1.5000, -0.5000]])"]},"metadata":{},"execution_count":55}],"source":["torch.inverse(A)"]},{"cell_type":"markdown","metadata":{"id":"kveC2ENiczPd"},"source":["简单试探逆矩阵的基本特性"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5Cap_4kczPd","executionInfo":{"status":"ok","timestamp":1648572415032,"user_tz":-120,"elapsed":272,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"04595122-ca8c-438f-a749-e9f4dd8e1943"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.0000e+00, -5.9605e-08],\n","        [-1.1921e-07,  1.0000e+00]])"]},"metadata":{},"execution_count":56}],"source":["torch.mm(torch.inverse(A), A)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quQaPlboczPd","executionInfo":{"status":"ok","timestamp":1648572422614,"user_tz":-120,"elapsed":305,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"741a847b-d197-4a40-b47c-6ac4412bedef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.0000e+00, -5.9605e-08],\n","        [-1.1921e-07,  1.0000e+00]])"]},"metadata":{},"execution_count":57}],"source":["torch.mm(A, torch.inverse(A))"]},{"cell_type":"markdown","metadata":{"id":"2uwqeGnKczPe"},"source":["然后在方程组左右两边同时左乘$A^{-1}$，求解x      \n","<center> $A^{-1} * A * x= A^{-1} * B $ </center>      \n","<center>$ E * x = A^{-1} * B $  </center>     \n","<center>$ x = A^{-1} * B$ </center> "]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BbdQqsIczPe","executionInfo":{"status":"ok","timestamp":1648572553513,"user_tz":-120,"elapsed":255,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"9546f59b-8404-4a51-eb71-025e1e1b771a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.0000, 1.0000])"]},"metadata":{},"execution_count":58}],"source":["torch.mv(torch.inverse(A), B)"]},{"cell_type":"markdown","metadata":{"id":"Y9tL6WCMczPe"},"source":["最终得到线性方程为："]},{"cell_type":"markdown","metadata":{"id":"esBEsIg6czPe"},"source":["<center>$ y = x + 1$ </center> "]},{"cell_type":"markdown","metadata":{"id":"U8THarYnczPe"},"source":["当然，上述计算过程只是一个简化的线性方程组求解系数的过程，同时也是一个简单的一元线性方程拟合数据的过程，关于常用求解线性方程组系数的最小二乘法，可以先阅读本节末尾的选读内容，更多线性回归相关内容，我们将在下周进行详细讲解。     "]},{"cell_type":"markdown","metadata":{"id":"lvDptpktczPe"},"source":["## 五、矩阵的分解"]},{"cell_type":"markdown","metadata":{"id":"9J_wb_LBczPe"},"source":["&emsp;&emsp;矩阵的分解也是矩阵运算中的常规计算，矩阵分解也有很多种类，常见的例如QR分解、LU分解、特征分解、SVD分解等等等等，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积，但本质上，矩阵的分解是去探索矩阵更深层次的一些属性。本节将主要围绕特征分解和SVD分解展开讲解，更多矩阵分解的运算，我们将在后续课程中逐渐进行介绍。值得一提的是，此前的逆矩阵，其实也可以将其看成是一种矩阵分解的方式，分解之后的等式如下：      \n","<center> $A = A * A^{-1} * A $ </center>      \n","而大多数情况下，矩阵分解都是分解成形如下述形式      \n","<center> $ A = VUD$ </center>   "]},{"cell_type":"markdown","metadata":{"id":"VLygkn6IczPf"},"source":["### 1.特征分解"]},{"cell_type":"markdown","metadata":{"id":"HYnroQb3czPf"},"source":["特征分解中，矩阵分解形式为：       \n","<center> $ A = Q\\Lambda Q^{-1}$ </center>         \n","其中，Q和$Q^{-1}$互为逆矩阵，并且Q的列就是A的特征值所对应的特征向量，而$\\Lambda$为矩阵A的特征值按照降序排列组成的对角矩阵。"]},{"cell_type":"markdown","metadata":{"id":"F30H1BOIczPf"},"source":["- torch.eig函数：特征分解"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K35vKCrCczPf","executionInfo":{"status":"ok","timestamp":1648572574309,"user_tz":-120,"elapsed":355,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"85d4fb62-70ca-41fa-c2a5-6b4255a6d3c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])"]},"metadata":{},"execution_count":59}],"source":["A = torch.arange(1, 10).reshape(3, 3).float()\n","A"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoWsEhp_czPf","executionInfo":{"status":"ok","timestamp":1648572574596,"user_tz":-120,"elapsed":27,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"3f69a8c9-680c-45b8-b490-20c7ad037b80"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n","torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n","L, _ = torch.eig(A)\n","should be replaced with\n","L_complex = torch.linalg.eigvals(A)\n","and\n","L, V = torch.eig(A, eigenvectors=True)\n","should be replaced with\n","L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2894.)\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.return_types.eig(eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],\n","        [-1.1168e+00,  0.0000e+00],\n","        [ 2.9486e-07,  0.0000e+00]]), eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],\n","        [-0.5253, -0.0868, -0.8165],\n","        [-0.8187,  0.6123,  0.4082]]))"]},"metadata":{},"execution_count":60}],"source":["torch.eig(A, eigenvectors=True)                 # 注，此处需要输入参数为True才会返回矩阵的特征向量"]},{"cell_type":"markdown","metadata":{"id":"vghkR7i2czPf"},"source":["输出结果中，eigenvalues表示特征值向量，即A矩阵分解后的Λ矩阵的对角线元素值，并按照又大到小依次排列，eigenvectors表示A矩阵分解后的Q矩阵，此处需要理解特征值，所谓特征值，可简单理解为对应列在矩阵中的信息权重，如果该列能够简单线性变换来表示其他列，则说明该列信息权重较大，反之则较小。特征值概念和秩的概念有点类似，但不完全相同，矩阵的秩表示矩阵列向量的最大线性无关数，而特征值的大小则表示某列向量能多大程度解读矩阵列向量的变异度，即所包含信息量，秩和特征值关系可用下面这个例子来进行解读。"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBVpGBOcczPg","executionInfo":{"status":"ok","timestamp":1648572574597,"user_tz":-120,"elapsed":26,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"f899592f-a5fc-4a6f-bcee-825dd1d890c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [2., 4.]])"]},"metadata":{},"execution_count":61}],"source":["B = torch.tensor([1, 2, 2, 4]).reshape(2, 2).float()\n","B"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWIrUxJzczPg","executionInfo":{"status":"ok","timestamp":1648572574597,"user_tz":-120,"elapsed":24,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"7a681233-ae82-47b6-95de-aabb63fbc230"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1)"]},"metadata":{},"execution_count":62}],"source":["torch.matrix_rank(B)"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3XAScV7czPg","executionInfo":{"status":"ok","timestamp":1648572574598,"user_tz":-120,"elapsed":24,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"22e7fb0e-be27-4655-ab68-e820b14a382d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.eig(eigenvalues=tensor([[0., 0.],\n","        [5., 0.]]), eigenvectors=tensor([]))"]},"metadata":{},"execution_count":63}],"source":["torch.eig(B)          # 返回结果中只有一个特征"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rq8NEDeczPh","executionInfo":{"status":"ok","timestamp":1648572574599,"user_tz":-120,"elapsed":24,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"9b345a0f-a3e4-4d65-f9df-4e3b0615fe78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [2., 4., 6.],\n","        [3., 6., 9.]])"]},"metadata":{},"execution_count":64}],"source":["C = torch.tensor([[1, 2, 3], [2, 4, 6], [3, 6, 9]]).float()\n","C"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ur_8QOT-czPh","executionInfo":{"status":"ok","timestamp":1648572574600,"user_tz":-120,"elapsed":24,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"83e7e25c-b829-4272-8bcb-c4d70f67375c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.eig(eigenvalues=tensor([[ 1.4000e+01,  0.0000e+00],\n","        [ 6.2356e-08,  0.0000e+00],\n","        [-2.8243e-07,  0.0000e+00]]), eigenvectors=tensor([]))"]},"metadata":{},"execution_count":65}],"source":["torch.eig(C)                # 只有一个特征的有效值"]},{"cell_type":"markdown","metadata":{"id":"j1oTTROVczPh"},"source":["特征值一般用于表示矩阵对应线性方程组解空间以及数据降维，当然，由于特征分解只能作用于方阵，而大多数实际情况下矩阵行列数未必相等，此时要进行类似的操作就需要采用和特征值分解思想类似的奇异值分解（SVD）。"]},{"cell_type":"markdown","metadata":{"id":"SPp2_Ht-czPi"},"source":["### 2.奇异值分解（SVD）"]},{"cell_type":"markdown","metadata":{"id":"XfYnfk4HczPi"},"source":["&emsp;&emsp;奇异值分解（SVD）来源于代数学中的矩阵分解问题，对于一个方阵来说，我们可以利用矩阵特征值和特征向量的特殊性质（矩阵点乘特征向量等于特征值数乘特征向量），通过求特征值与特征向量来达到矩阵分解的效果     \n","<center> $ A = Q\\Lambda Q^{-1}$ </center>            \n","这里，Q是由特征向量组成的矩阵，而Λ是特征值降序排列构成的一个对角矩阵（对角线上每个值是一个特征值，按降序排列，其他值为0），特征值的数值表示对应的特征的重要性。      \n","在很多情况下，最大的一小部分特征值的和即可以约等于所有特征值的和，而通过矩阵分解的降维就是通过在Q、Λ 中删去那些比较小的特征值及其对应的特征向量，使用一小部分的特征值和特征向量来描述整个矩阵，从而达到降维的效果。      \n","但是，实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为m×n的奇异矩阵A分解成三个部分 :      \n","<center> $ A = U\\sum V^{T}$ </center>         \n","其中U、V是两个正交矩阵，其中的每一行（每一列）分别被称为左奇异向量和右奇异向量，他们和∑中对角线上的奇异值相对应，通常情况下我们只需要保留前k个奇异向量和奇异值即可，其中U是m×k矩阵，V是n×k矩阵，∑是k×k的方阵，从而达到减少存储空间的效果，即      \n","<center> $ A_{m*n} = U_{m*m}\\sum_{m*n}V^{T}_{n*n}\\approx U_{m*k}\\sum_{k*k}V^{T}_{k*n}$ </center>   "]},{"cell_type":"markdown","metadata":{"id":"7j3tSta-czPi"},"source":["- svd奇异值分解函数"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8h654zuczPi","executionInfo":{"status":"ok","timestamp":1648572574601,"user_tz":-120,"elapsed":22,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"afa3af10-35a9-4f42-f4bb-854993940059"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [2., 4., 6.],\n","        [3., 6., 9.]])"]},"metadata":{},"execution_count":66}],"source":["C"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDDRYMEpczPj","executionInfo":{"status":"ok","timestamp":1648572574601,"user_tz":-120,"elapsed":21,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"48fc4d10-4bc4-48fa-b0dc-4063a6cddcf0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.svd(U=tensor([[-2.6726e-01,  9.6362e-01, -3.7767e-08],\n","        [-5.3452e-01, -1.4825e-01, -8.3205e-01],\n","        [-8.0178e-01, -2.2237e-01,  5.5470e-01]]), S=tensor([1.4000e+01, 4.2751e-08, 1.6397e-15]), V=tensor([[-0.2673, -0.9636,  0.0000],\n","        [-0.5345,  0.1482, -0.8321],\n","        [-0.8018,  0.2224,  0.5547]]))"]},"metadata":{},"execution_count":67}],"source":["torch.svd(C)"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"A27Z72B9czPj","executionInfo":{"status":"ok","timestamp":1648572574602,"user_tz":-120,"elapsed":19,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["CU, CS, CV = torch.svd(C)"]},{"cell_type":"markdown","metadata":{"id":"_k1O2z6FczPj"},"source":["验证SVD分解"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cS5_j68czPj","executionInfo":{"status":"ok","timestamp":1648572574603,"user_tz":-120,"elapsed":19,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"7b4de0bf-6783-45e9-fe73-f05e0555e6bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.4000e+01, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 4.2751e-08, 0.0000e+00],\n","        [0.0000e+00, 0.0000e+00, 1.6397e-15]])"]},"metadata":{},"execution_count":69}],"source":["torch.diag(CS)"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fbajR2jczPj","executionInfo":{"status":"ok","timestamp":1648572574603,"user_tz":-120,"elapsed":17,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"52907e32-be40-449c-e23e-be8182de28c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 2.0000, 3.0000],\n","        [2.0000, 4.0000, 6.0000],\n","        [3.0000, 6.0000, 9.0000]])"]},"metadata":{},"execution_count":70}],"source":["torch.mm(torch.mm(CU, torch.diag(CS)), CV.t())"]},{"cell_type":"markdown","metadata":{"id":"YyreJp1NczPk"},"source":["能够看出，上述输出完整还原了C矩阵，此时我们可根据svd输出结果对C进行降维，此时C可只保留第一列（后面的奇异值过小），即k=1  "]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkA0iBKsczPk","executionInfo":{"status":"ok","timestamp":1648572574604,"user_tz":-120,"elapsed":16,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"61c19c28-743b-415d-8223-963fa5db4183"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.2673],\n","        [-0.5345],\n","        [-0.8018]])"]},"metadata":{},"execution_count":71}],"source":["U1 = CU[:, 0].reshape(3, 1)         # U的第一列\n","U1"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Q89fmAeczPk","executionInfo":{"status":"ok","timestamp":1648572574882,"user_tz":-120,"elapsed":292,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"93660bdf-29bc-4bda-9c92-a4def00b7682"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14.0000)"]},"metadata":{},"execution_count":72}],"source":["C1 = CS[0]                           # C的第一个值\n","C1"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqdJ1D9_czPk","executionInfo":{"status":"ok","timestamp":1648572574884,"user_tz":-120,"elapsed":21,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"1dd110fa-7c42-40eb-bae9-f8a5233a9f92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.2673, -0.5345, -0.8018]])"]},"metadata":{},"execution_count":73}],"source":["V1 = CV[:, 0].reshape(1, 3)           # V的第一行\n","V1"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fTiLeMIczPk","executionInfo":{"status":"ok","timestamp":1648572574885,"user_tz":-120,"elapsed":19,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"dafd1ff4-03e7-4571-8061-4faef7a07a35"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 2.0000, 3.0000],\n","        [2.0000, 4.0000, 6.0000],\n","        [3.0000, 6.0000, 9.0000]])"]},"metadata":{},"execution_count":74}],"source":["torch.mm((U1 * C1), V1)"]},{"cell_type":"markdown","metadata":{"id":"_OUi4UTWczPk"},"source":["此时输出的Cd矩阵已经和原矩阵C高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫。"]},{"cell_type":"markdown","metadata":{"id":"tmgseixwczPl"},"source":["**本节选读内容**"]},{"cell_type":"markdown","metadata":{"id":"RHQ7Kr1-czPl"},"source":["> 另外，我们需要知道的是，除了利用逆矩阵求解线性方程组系数外，比较通用的方法是使用最小二乘法进行求解："]},{"cell_type":"markdown","metadata":{"id":"Yy-1qaxQczPl"},"source":["- torch.lstsq：最小二乘法"]},{"cell_type":"markdown","metadata":{"id":"nySDFC5KczPl"},"source":["&emsp;&emsp;最小二乘法是最通用的线性方程拟合求解工具，我们可以利用最小二乘法的直接计算拟合直线的系数最优解。当然，本节仅介绍最小二乘法的函数调用，下节在介绍目标函数和优化手段时，还将进一步介绍最小二乘法的数学原理。"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"zHHYdzuwczPl","executionInfo":{"status":"error","timestamp":1648572574887,"user_tz":-120,"elapsed":19,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}},"outputId":"a1dcd1c9-246b-4a65-cac7-1e2ab85b5b56"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-8237fcf4f297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 1]' is invalid for input of size 4"]}],"source":["torch.lstsq(B.reshape(2, 1), A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xx420OBvczPm","executionInfo":{"status":"aborted","timestamp":1648572574885,"user_tz":-120,"elapsed":14,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["x, q = torch.lstsq(B.reshape(2, 1), A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOafk5GyczPm","executionInfo":{"status":"aborted","timestamp":1648572574885,"user_tz":-120,"elapsed":13,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STbUS1kUczPm","executionInfo":{"status":"aborted","timestamp":1648572574886,"user_tz":-120,"elapsed":14,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["q"]},{"cell_type":"markdown","metadata":{"id":"9Q4pjVLkczPm"},"source":["我们发现，最小二乘法返回了两个结果，分别是x的系数和QR分解后的QR矩阵。"]},{"cell_type":"markdown","metadata":{"id":"7mS2s9_LczPm"},"source":["- solve函数与LU分解"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rifQU6MyczPn","executionInfo":{"status":"aborted","timestamp":1648572574886,"user_tz":-120,"elapsed":14,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["torch.solve(B.reshape(2, 1), A)"]},{"cell_type":"markdown","metadata":{"id":"I9TdURTLczPn"},"source":["- LU分解函数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkRKlmQ5czPo","executionInfo":{"status":"aborted","timestamp":1648572574886,"user_tz":-120,"elapsed":14,"user":{"displayName":"李镜璇","userId":"01439314030893178341"}}},"outputs":[],"source":["torch.lu(A)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Lesson 4.张量的线性代数运算.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}